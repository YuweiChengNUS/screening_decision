summary(boostmodel4)
dev.off()
set.seed(1)
library(readxl)
library(fastDummies)
library(lmtest)
library(forestplot)
library(caret)
library(e1071)
library(gbm)
library(MASS)
library(ROCit)
library(Amelia)
source("code/supporting_functions.R")
source("code/step1_baseline_model.R")
source("code/plot_logistic.R")
source("code/step2_machine_learning_models.R")
head(data_naomit)
obs_matrix
obs_matrix = read.csv("output/obs_matrix.csv",header = TRUE)[,-1]
mean_matrix = read.csv("output/mean_matrix.csv",header = TRUE)[,-1]
na_matrix = read.csv("output/na_matrix.csv",header = TRUE)[,-1]
na_matrix
mean_matrix
obs_matrix
p_matrix = read.csv("output/p_matrix.csv",header = TRUE)[,-1]
p_matrix = read.csv("output/representative_test.csv",header = TRUE)[,-1]
i = 1
cat(i,"&",obs_matrix[i],"&",mean_matrix[1,1],"(",na_matrix[1,1],")(",p_matrix[1,1],")")
dim(mean_matrix)
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&",
mean_matrix[i,2],"(",na_matrix[i,2],")(",p_matrix[i,2],")&",
mean_matrix[i,3],"(",na_matrix[i,3],")(",p_matrix[i,3],")&",
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&",
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&",
mean_matrix[i,6],"(",na_matrix[i,6],")(",p_matrix[i,6],")&",
mean_matrix[i,7],"(",na_matrix[i,7],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,8],")(",p_matrix[i,8],")\\")
cat("\n")
}
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&",
mean_matrix[i,2],"(",na_matrix[i,2],")(",p_matrix[i,2],")&",
mean_matrix[i,3],"(",na_matrix[i,3],")(",p_matrix[i,3],")&",
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&",
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&",
mean_matrix[i,6],"(",na_matrix[i,6],")(",p_matrix[i,6],")&",
mean_matrix[i,7],"(",na_matrix[i,7],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,8],")(",p_matrix[i,8],")\\\\")
cat("\n")
}
na_matrix
mean_matrix
p_matrix
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&",
mean_matrix[i,2],"(",na_matrix[i,2],")(",p_matrix[i,2],")&",
mean_matrix[i,3],"(",na_matrix[i,3],")(",p_matrix[i,3],")&",
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&",
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&",
mean_matrix[i,6],"(",na_matrix[i,7],")(",p_matrix[i,6],")&",
mean_matrix[i,7],"(",na_matrix[i,6],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,6],")(",p_matrix[i,8],")\\\\")
cat("\n")
}
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&",
mean_matrix[i,2],"(",na_matrix[i,2],")(",p_matrix[i,2],")&",
mean_matrix[i,3],"(",na_matrix[i,3],")(",p_matrix[i,3],")&",
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&",
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&",
mean_matrix[i,6],"(",na_matrix[i,7],")(",p_matrix[i,6],")&",
mean_matrix[i,7],"(",na_matrix[i,6],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,6],")(",p_matrix[i,8],")\\\\",sep = "")
cat("\n")
}
mean_matrix
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&",
mean_matrix[i,2],")(",p_matrix[i,2],")&", # age
mean_matrix[i,3],")(",p_matrix[i,3],")&", # weight
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&", # height
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&", # alcohol
mean_matrix[i,6],")(",p_matrix[i,6],")&", # deps
mean_matrix[i,7],"(",na_matrix[i,6],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,6],")(",p_matrix[i,8],")\\\\",sep = "")
cat("\n")
}
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&(",
mean_matrix[i,2],")(",p_matrix[i,2],")&", # age
mean_matrix[i,3],")(",p_matrix[i,3],")&", # weight
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&", # height
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&", # alcohol
mean_matrix[i,6],")(",p_matrix[i,6],")&", # deps
mean_matrix[i,7],"(",na_matrix[i,6],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,6],")(",p_matrix[i,8],")\\\\",sep = "")
cat("\n")
}
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",mean_matrix[i,1],"(",na_matrix[i,1],")(",p_matrix[i,1],")&",
mean_matrix[i,2],"(",p_matrix[i,2],")&", # age
mean_matrix[i,3],"(",p_matrix[i,3],")&", # weight
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&", # height
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&", # alcohol
mean_matrix[i,6],"(",p_matrix[i,6],")&", # deps
mean_matrix[i,7],"(",na_matrix[i,6],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",na_matrix[i,6],")(",p_matrix[i,8],")\\\\",sep = "")
cat("\n")
}
for(i in 1:12)
{
cat(i,"&",obs_matrix[i],"&",
mean_matrix[i,2],"(",p_matrix[i,2],")&", # age
mean_matrix[i,3],"(",p_matrix[i,3],")&", # weight
mean_matrix[i,4],"(",na_matrix[i,4],")(",p_matrix[i,4],")&", # height
mean_matrix[i,5],"(",na_matrix[i,5],")(",p_matrix[i,5],")&", # alcohol
mean_matrix[i,6],"(",p_matrix[i,6],")&", # deps
mean_matrix[i,7],"(",na_matrix[i,6],")(",p_matrix[i,7],")&",
mean_matrix[i,8],"(",p_matrix[i,8],")\\\\",sep = "")
cat("\n")
}
set.seed(1)
library(readxl)
library(fastDummies)
library(lmtest)
library(forestplot)
library(caret)
library(e1071)
library(gbm)
library(MASS)
library(ROCit)
library(Amelia)
table(ldaclass,validate$screening)
predictions = ifelse(ldapred>0.5,1,0)
predictions = as.factor(predictions)
is_screen = test$is_screen
is_screen = test(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
predictions = ifelse(ldapred>0.5,1,0)
predictions = as.factor(predictions)
is_screen = test$is_screen
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
predictions = ifelse(ldapred>0.5,1,0)
predictions = as.factor(predictions)
is_screen = validate$is_screen
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
sample_size = dim(data_naomit)[1]
index = sample(sample_size,sample_size,replace = FALSE)
train_size = floor(0.6*sample_size)
validation_size = floor(0.2*sample_size)
test_size = sample_size - train_size - validation_size
train = data_naomit[index[1:train_size],]
validate = data_naomit[index[(train_size+1):(train_size+validation_size)],]
test = data_naomit[index[(train_size+validation_size+1):sample_size],]
# Linear Discriminant Analysis
ldamodel=lda(screening~Age + Alcohol + Height + Weight + Deps + Full + Part + Team2 + Team3
+ Team4 + Team5 + Team6 + Team7 + Team8 + Team9 + Team10 + Team11 + Full:Deps + Part:Deps + BMI
,data=train)
ldapred = predict(ldamodel,validate)
predictions = ifelse(ldapred>0.5,1,0)
predictions = as.factor(predictions)
is_screen = validate$is_screen
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
predictions
is_screen
length(is_screen)
length(ldapred)
length(predictions)
ldapred
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,1,0)
predictions = as.factor(predictions)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
ldapred = predict(ldamodel,test)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,1,0)
predictions = as.factor(predictions)
is_screen = test$is_screen
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
ldapred = predict(ldamodel,test)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,1,0)
predictions = as.factor(predictions)
is_screen = test$is_screen
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
ldapred = predict(ldamodel,data_naomit)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,1,0)
predictions = as.factor(predictions)
is_screen = data_naomit$is_screen
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True"))
?confusionMatrix
confusionMatrix(predictions, is_screen, positive = 1, dnn = c("Prediction", "True"),)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = as.factor(predictions)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = as.factor(predictions)
is_screen = data_naomit$screening
is_screen = as.factor(is_screen)
confusionMatrix(predictions, is_screen, positive = "Y", dnn = c("Prediction", "True"),)
mean(data_naomit$screening=="Y")
predictions = ifelse(model1$fitted.values>0.5,"Y","N")
predictions = as.factor(predictions)
screening = data_naomit$screening
screening = as.factor(screening )
out = capture.output(confusionMatrix(predictions, is_screen, positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/baseline_confusion_matrix.txt")
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = as.factor(predictions)
screening = data_naomit$screening
screening = as.factor(is_screen)
out = capture.output(confusionMatrix(predictions, is_screen, positive = NULL, dnn = c("Prediction", "True")))
write.table(out,"output/lda_confusion_matrix.txt")
predictions = ifelse(predict(boostmodel4,newdata=data_naomit,n.trees=5000,type = "response")>0.5,"Y","N")
predictions = as.factor(predictions)
screening = data_naomit$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening, positive = NULL, dnn = c("Prediction", "True")))
write.table(out,"output/boosting_confusion_matrix.txt")
predictions = ifelse(predict(boostmodel4,newdata=data_naomit,n.trees=5000,type = "response")>0.5,"Y","N")
predictions = as.factor(predictions)
screening = data_naomit$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening, positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/boosting_confusion_matrix.txt")
predictions = ifelse(predict(boostmodel4,newdata=test,n.trees=5000,type = "response")>0.5,"Y","N")
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening, positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/boosting_confusion_matrix.txt")
ldapred = predict(ldamodel,test)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening , positive = NULL, dnn = c("Prediction", "True")))
write.table(out,"output/lda_confusion_matrix.txt")
ldapred = predict(ldamodel,test)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/lda_confusion_matrix.txt")
predictions = glmpred
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/glm_confusion_matrix.txt")
boostingpred = ifelse(predict(boostmodel4,newdata=test,n.trees=5000,type = "response")>0.5,"Y","N")
ldapred = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = ifelse(glmpred=="Y" || ldapred=="Y" || boostingpred=="Y","Y","N")
confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True"))
predictions = as.factor(predictions)
confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True"))
predictions
glmpred=
1)
glmpred[glmprobs>.5]="Y"
glmpred
ldapred = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
ldapred = predict(ldamodel,test)
ldapred1 = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
ldapred1
boostingpred
ifelse(glmpred=="Y" || ldapred=="Y" || boostingpred=="Y","Y","N")
glmpred=="Y"
ldapred=="Y"
ldapred1=="Y"
ifelse(glmpred=="Y" || ldapred1=="Y" || boostingpred=="Y","Y","N")
boostingpred=="Y"
glmpred=="Y" || ldapred1=="Y" || boostingpred=="Y"
cbind(glmpred=="Y", ldapred1=="Y",boostingpred=="Y")
apply(cbind(glmpred=="Y", ldapred1=="Y",boostingpred=="Y"),2,all)
apply(cbind(glmpred=="Y", ldapred1=="Y",boostingpred=="Y"),1,all)
apply(cbind(glmpred=="N", ldapred1=="N",boostingpred=="N"),1,all)
predictions = ifelse(apply(cbind(glmpred=="N", ldapred1=="N",boostingpred=="N"),1,all),"N","Y")
predictions
predictions = as.factor(predictions)
confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True"))
check = function(a,b,c)
{
return(a||b||c)
}
predictions = ifelse(apply(cbind(glmpred=="Y", ldapred1=="Y",boostingpred=="Y"),1,check),"Y","N")
predictions = as.factor(predictions)
check(glmpred,ldapred1,boostingpred)
length(glmpred)
for(i in 1:97){predictions[i] = ifelse(check(glmpred[i],ldapred1[i],boostingpred[i]),"Y","N")}
check = function(a,b,c)
{
return(a=="Y" ||b=="Y" ||c=="Y" )
}
for(i in 1:97){predictions[i] = ifelse(check(glmpred[i],ldapred1[i],boostingpred[i]),"Y","N")}
predictions = as.factor(predictions)
confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True"))
sample_size = dim(data_naomit)[1]
index = sample(sample_size,sample_size,replace = FALSE)
train_size = floor(0.6*sample_size)
validation_size = floor(0.2*sample_size)
test_size = sample_size - train_size - validation_size
train = data_naomit[index[1:train_size],]
validate = data_naomit[index[(train_size+1):(train_size+validation_size)],]
test = data_naomit[index[(train_size+validation_size+1):sample_size],]
# Linear Discriminant Analysis
ldamodel=lda(screening~Age + Alcohol + Height + Weight + Deps + Full + Part + Team2 + Team3
+ Team4 + Team5 + Team6 + Team7 + Team8 + Team9 + Team10 + Team11 + Full:Deps + Part:Deps + BMI
,data=train)
ldapred = predict(ldamodel,validate)
ldaclass = ldapred$class
table(ldaclass,validate$screening)
mean(ldaclass==validate$screening)
# Confusion Matrix
ldapred = predict(ldamodel,test)
predictions = ifelse(ldapred$posterior[,"Y"]>0.5,"Y","N")
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/lda_confusion_matrix.txt")
# Boosting
boostmodel1 = gbm(is_screen~Age + Alcohol + Height + Weight + Deps + Full + Part + Team2 + Team3
+ Team4 + Team5 + Team6 + Team7 + Team8 + Team9 + Team10 + Team11 + BMI + Full:Deps + Part:Deps
,data = train,distribution = "bernoulli", n.trees = 5000, interaction.depth = 1,
shrinkage=0.01,verbose=F)
boostmodel2 = gbm(is_screen~Age + Alcohol + Height + Weight + Deps + Full + Part + Team2 + Team3
+ Team4 + Team5 + Team6 + Team7 + Team8 + Team9 + Team10 + Team11 + BMI+Full:Deps + Part:Deps
,data = train,distribution = "bernoulli", n.trees = 5000, interaction.depth = 2,
shrinkage=0.01,verbose=F)
boostmodel3 = gbm(is_screen~Age + Alcohol + Height + Weight + Deps + Full + Part + Team2 + Team3
+ Team4 + Team5 + Team6 + Team7 + Team8 + Team9 + Team10 + Team11 + BMI+Full:Deps + Part:Deps
,data = train,distribution = "bernoulli", n.trees = 5000, interaction.depth = 3,
shrinkage=0.01,verbose=F)
boostmodel4 = gbm(is_screen~Age + Alcohol + Height + Weight + Deps + Full + Part + Team2 + Team3
+ Team4 + Team5 + Team6 + Team7 + Team8 + Team9 + Team10 + Team11 + BMI+Full:Deps + Part:Deps
,data = train,distribution = "bernoulli", n.trees = 5000, interaction.depth = 4,
shrinkage=0.01,verbose=F)
yhat_boost1=ifelse(predict(boostmodel1,newdata=validate,n.trees=5000,type = "response")>0.5,1,0)
yhat_boost2=ifelse(predict(boostmodel2,newdata=validate,n.trees=5000,type = "response")>0.5,1,0)
yhat_boost3=ifelse(predict(boostmodel3,newdata=validate,n.trees=5000,type = "response")>0.5,1,0)
yhat_boost4=ifelse(predict(boostmodel4,newdata=validate,n.trees=5000,type = "response")>0.5,1,0)
mean(yhat_boost1 == validate$is_screen)
mean(yhat_boost2 == validate$is_screen)
mean(yhat_boost3 == validate$is_screen)
mean(yhat_boost4 == validate$is_screen)
summary(boostmodel4)
# Confusion Matrix
predictions = ifelse(predict(boostmodel4,newdata=test,n.trees=5000,type = "response")>0.5,"Y","N")
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening, positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/boosting_confusion_matrix.txt")
# Logistic Regression
glmmodel=glm(formula(bothways),data=train,family = binomial)
summary(glmmodel)
glmprobs=predict(glmmodel,test,type="response")
glmpred=rep("N",test_size)
glmpred[glmprobs>.5]="Y"
mean(glmpred == test$screening)
table(glmpred,test$screening)
# Confusion Matrix
predictions = glmpred
predictions = as.factor(predictions)
screening = test$screening
screening = as.factor(screening)
out = capture.output(confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True")))
write.table(out,"output/glm_confusion_matrix.txt")
# Plot ROC
glmroc = rocit(score=glmprobs,class=test$is_screen)
glmroc$AUC
pdf("plot/roc.pdf",height = 1.9685*2,width = 1.9685*2,pointsize = 12)
plot(glmroc$FPR,glmroc$TPR,xlab = "1-Specificity(FPR)",ylab = "Sensitivity(TPR)",las = 1,type = "l",
main = "Performance Comparison on Test Dataset",cex.main = 0.8, cex.lab= 0.8, cex.axis = 0.8)
yhat_boost4=ifelse(predict(boostmodel4,newdata=test,n.trees=5000,type = "response")>0.5,1,0)
boostroc = rocit(score=yhat_boost4,class=test$is_screen)
boostroc$AUC
lines(boostroc$FPR,boostroc$TPR,col = "red")
ldapred = ifelse(predict(ldamodel,test)$posterior[,2]>0.5,1,0)
ldaroc = rocit(score=ldapred,class=test$is_screen)
ldaroc$AUC
lines(ldaroc$FPR,ldaroc$TPR,col = "blue")
abline(a = 0,b = 1,lty = 2)
legend("bottomright", legend=c("Logistic Regression (AUC = 0.86)", "Boosting (AUC = 0.94)","LDA (AUC = 0.94)"),
col=c("black","red","blue"), lty=rep(1,3), cex=0.6)
dev.off()
# Marginal Effects and Influence
pdf("plot/boost.pdf",pointsize = 3)
summary(boostmodel4)
dev.off()
check = function(a,b,c)
{
return(a==1 ||b==1 ||c==1 )
}
for(i in 1:97){predictions[i] = ifelse(check(glmpred[i],ldapred[i],yhat_boost4[i]),"Y","N")}
predictions = as.factor(predictions)
confusionMatrix(predictions, screening , positive = "Y", dnn = c("Prediction", "True"))
set.seed(1)
library(readxl)
library(fastDummies)
library(lmtest)
library(forestplot)
library(caret)
library(e1071)
library(gbm)
library(MASS)
library(ROCit)
library(Amelia)
set.seed(1)
library(readxl)
library(fastDummies)
library(lmtest)
library(forestplot)
library(caret)
library(e1071)
library(gbm)
library(MASS)
library(ROCit)
library(Amelia)
source("code/supporting_functions.R")
source("code/step1_baseline_model.R")
source("code/plot_logistic.R")
source("code/step2_machine_learning_models.R")
pdf("plot/age.pdf")
plot(boostmodel4,i="Age")
dev.off()
pdf("plot/alcohol.pdf")
plot(boostmodel4,i="Alcohol")
dev.off()
pdf("plot/BMI.pdf")
plot(boostmodel4,i="BMI")
dev.off()
pdf("plot/Deps.pdf")
plot(boostmodel4,i="Deps")
dev.off()
pdf("plot/Full.pdf")
plot(boostmodel4,i="Full")
dev.off()
pdf("plot/Part.pdf")
plot(boostmodel4,i="Part")
dev.off()
pdf("plot/Height.pdf")
plot(boostmodel4,i="Height")
dev.off()
pdf("plot/Weight.pdf")
plot(boostmodel4,i="Weight")
dev.off()
source("code/step0_create_python_dataset.R")
source("code/step3_imputation_model.R")
# import library
# install.packages("BaylorEdPsych")
# install.packages("mvnmle")
# install.packages("forestplot")
# install.packages("lmtest")
# install.packages("caret")
# install.packages("ROCit")
set.seed(1)
library(readxl)
library(fastDummies)
library(lmtest)
library(forestplot)
library(caret)
library(e1071)
library(gbm)
library(MASS)
library(ROCit)
library(Amelia)
source("code/supporting_functions.R")
source("code/step1_baseline_model.R")
source("code/plot_logistic.R")
source("code/step2_machine_learning_models.R")
pdf("plot/age.pdf")
plot(boostmodel4,i="Age")
dev.off()
pdf("plot/alcohol.pdf")
plot(boostmodel4,i="Alcohol")
dev.off()
pdf("plot/BMI.pdf")
plot(boostmodel4,i="BMI")
dev.off()
pdf("plot/Deps.pdf")
plot(boostmodel4,i="Deps")
dev.off()
pdf("plot/Full.pdf")
plot(boostmodel4,i="Full")
dev.off()
pdf("plot/Part.pdf")
plot(boostmodel4,i="Part")
dev.off()
pdf("plot/Height.pdf")
plot(boostmodel4,i="Height")
dev.off()
pdf("plot/Weight.pdf")
plot(boostmodel4,i="Weight")
dev.off()
source("code/step0_create_python_dataset.R")
source("code/step3_imputation_model.R")
0.54*14+11
0.07*14+17.6
